from typing import List, Optional, Any, Dict
from pydantic import BaseModel, Field
from datetime import datetime

class TestCase(BaseModel):
    """
    The standard unit of evaluation.
    Using Pydantic ensures data validation and prevents runtime errors.
    
    Enhanced version with support for:
    - Conversation history (multi-turn dialogues)
    - Ground truth facts (for factual consistency checks)
    - Expected references (for citation validation)
    - Rich metadata
    """
    input_text: str = Field(..., description="The original user query.")
    actual_output: str = Field(..., description="The response generated by the LLM.")
    retrieval_context: Optional[List[str]] = Field(default=None, description="Chunks retrieved from the vector DB.")
    expected_output: Optional[str] = Field(default=None, description="The 'Gold Standard' answer (optional).")
    
    # Enhanced fields for advanced evaluation
    conversation_history: Optional[List[Dict[str, str]]] = Field(
        default=None, 
        description="Previous turns in the conversation. Format: [{'role': 'user'/'assistant', 'content': '...'}]"
    )
    ground_truth_facts: Optional[List[str]] = Field(
        default=None,
        description="Known facts for factual consistency checking."
    )
    expected_references: Optional[List[str]] = Field(
        default=None,
        description="Expected citations or references that should appear in the output."
    )
    
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Extra tags like 'model_name' or 'latency'.")
    test_case_id: Optional[str] = Field(default=None, description="Unique identifier for this test case.")

class MetricResult(BaseModel):
    """
    Standardized output for any metric.
    """
    metric_name: str
    score: float
    reason: str
    passed: bool
    execution_time: Optional[float] = Field(default=None, description="Time taken to compute this metric (seconds).")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional metric-specific data.")

class EvaluationResult(BaseModel):
    """
    Comprehensive result object for a single evaluation.
    Contains all metric results plus aggregated information.
    """
    test_case_id: str
    input_text: str
    actual_output: str
    metric_results: Dict[str, MetricResult]
    aggregate_score: float = Field(description="Weighted average of all metric scores.")
    execution_time: float = Field(description="Total time for evaluation (seconds).")
    cost: Optional[float] = Field(default=None, description="Total cost in USD (if tracking enabled).")
    timestamp: datetime = Field(default_factory=datetime.now)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
class MetricConfig(BaseModel):
    """
    Configuration for individual metrics.
    """
    threshold: float = Field(default=0.5, ge=0.0, le=1.0, description="Pass/fail threshold.")
    weight: float = Field(default=1.0, ge=0.0, description="Weight for aggregation.")
    enabled: bool = Field(default=True, description="Whether this metric is active.")
    llm_model: Optional[str] = Field(default=None, description="Override LLM model for this metric.")